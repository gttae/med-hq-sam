{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gttae/med-hq-sam/blob/main/HQ_SAM_Medical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TVPOWraXnmUu"
      },
      "outputs": [],
      "source": [
        "#깃 클론\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "\n",
        "!git clone https://github.com/gttae/med-hq-sam.git\n",
        "!pip install timm\n",
        "os.chdir('med-hq-sam')\n",
        "!export PYTHONPATH=$(pwd)\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "#!mkdir pretrained_checkpoint\n",
        "#!wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth\n",
        "#!mv sam_hq_vit_l.pth pretrained_checkpoint\n",
        "\n",
        "sam_checkpoint = \"/content/drive/MyDrive/sam_hq_vit_l (1).pth\"\n",
        "model_type = \"vit_l\"\n",
        "#device = \"cuda\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFPY8taRnwTD",
        "outputId": "ae23638f-3bd1-44dd-af6f-f6e896cfdf65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Collecting connected-components-3d\n",
            "  Downloading connected_components_3d-3.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.4.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (24.0)\n",
            "Installing collected packages: SimpleITK, connected-components-3d\n",
            "Successfully installed SimpleITK-2.3.1 connected-components-3d-3.14.1\n"
          ]
        }
      ],
      "source": [
        "#데이터 전처리 모듈 다운\n",
        "!pip install SimpleITK scikit-image tqdm connected-components-3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqf9_IK_oAph",
        "outputId": "ea3e7271-2769-43a6-967d-8f553629c5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ori \\# files len(names)=41\n",
            "Available image files: ['.ipynb_checkpoints', 'FLARE22_Tr_0001_0000 (1).nii', 'FLARE22_Tr_0002_0000 (1).nii', 'FLARE22_Tr_0003_0000 (1).nii', 'FLARE22_Tr_0004_0000 (1).nii', 'FLARE22_Tr_0005_0000 (1).nii', 'FLARE22_Tr_0006_0000 (1).nii', 'FLARE22_Tr_0007_0000 (1).nii', 'FLARE22_Tr_0008_0000 (1).nii', 'FLARE22_Tr_0009_0000 (1).nii', 'FLARE22_Tr_0010_0000 (1).nii', 'FLARE22_Tr_0011_0000.nii', 'FLARE22_Tr_0012_0000.nii', 'FLARE22_Tr_0013_0000.nii', 'FLARE22_Tr_0014_0000.nii', 'FLARE22_Tr_0015_0000.nii', 'FLARE22_Tr_0016_0000.nii', 'FLARE22_Tr_0017_0000.nii', 'FLARE22_Tr_0018_0000.nii', 'FLARE22_Tr_0019_0000.nii', 'FLARE22_Tr_0020_0000.nii', 'FLARE22_Tr_0021_0000.nii', 'FLARE22_Tr_0022_0000.nii', 'FLARE22_Tr_0023_0000.nii', 'FLARE22_Tr_0024_0000.nii', 'FLARE22_Tr_0025_0000.nii', 'FLARE22_Tr_0026_0000.nii', 'FLARE22_Tr_0027_0000.nii', 'FLARE22_Tr_0028_0000.nii', 'FLARE22_Tr_0029_0000.nii', 'FLARE22_Tr_0030_0000.nii', 'FLARE22_Tr_0031_0000.nii', 'FLARE22_Tr_0032_0000.nii', 'FLARE22_Tr_0033_0000.nii', 'FLARE22_Tr_0034_0000.nii', 'FLARE22_Tr_0035_0000.nii', 'FLARE22_Tr_0036_0000.nii', 'FLARE22_Tr_0037_0000.nii', 'FLARE22_Tr_0038_0000.nii', 'FLARE22_Tr_0039_0000.nii', 'FLARE22_Tr_0041_0000.nii']\n",
            "Filtered names with corresponding images: ['FLARE22_Tr_0011.nii', 'FLARE22_Tr_0012.nii', 'FLARE22_Tr_0013.nii', 'FLARE22_Tr_0014.nii', 'FLARE22_Tr_0015.nii', 'FLARE22_Tr_0016.nii', 'FLARE22_Tr_0017.nii', 'FLARE22_Tr_0018.nii', 'FLARE22_Tr_0019.nii', 'FLARE22_Tr_0020.nii', 'FLARE22_Tr_0021.nii', 'FLARE22_Tr_0022.nii', 'FLARE22_Tr_0023.nii', 'FLARE22_Tr_0024.nii', 'FLARE22_Tr_0025.nii', 'FLARE22_Tr_0026.nii', 'FLARE22_Tr_0027.nii', 'FLARE22_Tr_0028.nii', 'FLARE22_Tr_0029.nii', 'FLARE22_Tr_0030.nii', 'FLARE22_Tr_0031.nii', 'FLARE22_Tr_0032.nii', 'FLARE22_Tr_0033.nii', 'FLARE22_Tr_0034.nii', 'FLARE22_Tr_0035.nii', 'FLARE22_Tr_0036.nii', 'FLARE22_Tr_0037.nii', 'FLARE22_Tr_0038.nii', 'FLARE22_Tr_0039.nii', 'FLARE22_Tr_0041.nii']\n",
            "after sanity check \\# files len(names)=30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [1:38:04<00:00, 196.14s/it]\n"
          ]
        }
      ],
      "source": [
        "#데이터 전처리 코드\n",
        "# -*- coding: utf-8 -*-\n",
        "# %% import packages\n",
        "# pip install connected-components-3d\n",
        "import numpy as np\n",
        "# import nibabel as nib\n",
        "import SimpleITK as sitk\n",
        "import os\n",
        "\n",
        "join = os.path.join\n",
        "from skimage import transform\n",
        "from tqdm import tqdm\n",
        "import cc3d\n",
        "\n",
        "# convert nii image to npz files, including original image and corresponding masks\n",
        "modality = \"CT\"\n",
        "anatomy = \"Abd\"  # anantomy + dataset name\n",
        "img_name_suffix = \"_0000.nii\"\n",
        "gt_name_suffix = \".nii\"\n",
        "prefix = modality + \"_\" + anatomy + \"_\"\n",
        "\n",
        "nii_path = \"/content/drive/MyDrive/FLARE22Train/images\"  # path to the nii images\n",
        "gt_path = \"/content/drive/MyDrive/FLARE22Train/labels\"  # path to the ground truth\n",
        "npy_path = \"/content/drive/MyDrive/npy/\" + prefix[:-1]\n",
        "os.makedirs(join(npy_path, \"gts\"), exist_ok=True)\n",
        "os.makedirs(join(npy_path, \"imgs\"), exist_ok=True)\n",
        "\n",
        "image_size = 1024\n",
        "voxel_num_thre2d = 100\n",
        "voxel_num_thre3d = 1000\n",
        "\n",
        "names = sorted(os.listdir(gt_path))\n",
        "print(f\"ori \\# files {len(names)=}\")\n",
        "names = [\n",
        "    name\n",
        "    for name in names\n",
        "    if os.path.exists(join(nii_path, name.split(gt_name_suffix)[0] + img_name_suffix))\n",
        "]\n",
        "# 이미지 파일 경로에서 파일 이름을 로드하고 출력\n",
        "image_files = sorted(os.listdir(nii_path))\n",
        "print(\"Available image files:\", image_files)\n",
        "\n",
        "# 라벨에 해당하는 이미지 파일이 존재하는지 검사\n",
        "names = [\n",
        "    name for name in names\n",
        "    if os.path.exists(join(nii_path, name.split(gt_name_suffix)[0] + img_name_suffix))\n",
        "]\n",
        "print(\"Filtered names with corresponding images:\", names)\n",
        "\n",
        "\n",
        "print(f\"after sanity check \\# files {len(names)=}\")\n",
        "\n",
        "\n",
        "\n",
        "# set label ids that are excluded\n",
        "remove_label_ids = [\n",
        "    12\n",
        "]  # remove deodenum since it is scattered in the image, which is hard to specify with the bounding box\n",
        "tumor_id = None  # only set this when there are multiple tumors; convert semantic masks to instance masks\n",
        "# set window level and width\n",
        "# https://radiopaedia.org/articles/windowing-ct\n",
        "WINDOW_LEVEL = 40  # only for CT images\n",
        "WINDOW_WIDTH = 400  # only for CT images\n",
        "\n",
        "# %% save preprocessed images and masks as npz files\n",
        "for name in tqdm(names[:40]):  # use the remaining 10 cases for validation\n",
        "    image_name = name.split(gt_name_suffix)[0] + img_name_suffix\n",
        "    gt_name = name\n",
        "    gt_sitk = sitk.ReadImage(join(gt_path, gt_name))\n",
        "    gt_data_ori = np.uint8(sitk.GetArrayFromImage(gt_sitk))\n",
        "    # remove label ids\n",
        "    for remove_label_id in remove_label_ids:\n",
        "        gt_data_ori[gt_data_ori == remove_label_id] = 0\n",
        "    # label tumor masks as instances and remove from gt_data_ori\n",
        "    if tumor_id is not None:\n",
        "        tumor_bw = np.uint8(gt_data_ori == tumor_id)\n",
        "        gt_data_ori[tumor_bw > 0] = 0\n",
        "        # label tumor masks as instances\n",
        "        tumor_inst, tumor_n = cc3d.connected_components(\n",
        "            tumor_bw, connectivity=26, return_N=True\n",
        "        )\n",
        "        # put the tumor instances back to gt_data_ori\n",
        "        gt_data_ori[tumor_inst > 0] = (\n",
        "            tumor_inst[tumor_inst > 0] + np.max(gt_data_ori) + 1\n",
        "        )\n",
        "\n",
        "    # exclude the objects with less than 1000 pixels in 3D\n",
        "    gt_data_ori = cc3d.dust(\n",
        "        gt_data_ori, threshold=voxel_num_thre3d, connectivity=26, in_place=True\n",
        "    )\n",
        "    # remove small objects with less than 100 pixels in 2D slices\n",
        "\n",
        "    for slice_i in range(gt_data_ori.shape[0]):\n",
        "        gt_i = gt_data_ori[slice_i, :, :]\n",
        "        # remove small objects with less than 100 pixels\n",
        "        # reason: fro such small objects, the main challenge is detection rather than segmentation\n",
        "        gt_data_ori[slice_i, :, :] = cc3d.dust(\n",
        "            gt_i, threshold=voxel_num_thre2d, connectivity=8, in_place=True\n",
        "        )\n",
        "    # find non-zero slices\n",
        "    z_index, _, _ = np.where(gt_data_ori > 0)\n",
        "    z_index = np.unique(z_index)\n",
        "\n",
        "    if len(z_index) > 0:\n",
        "        # crop the ground truth with non-zero slices\n",
        "        gt_roi = gt_data_ori[z_index, :, :]\n",
        "        # load image and preprocess\n",
        "        img_sitk = sitk.ReadImage(join(nii_path, image_name))\n",
        "        image_data = sitk.GetArrayFromImage(img_sitk)\n",
        "        # nii preprocess start\n",
        "        if modality == \"CT\":\n",
        "            lower_bound = WINDOW_LEVEL - WINDOW_WIDTH / 2\n",
        "            upper_bound = WINDOW_LEVEL + WINDOW_WIDTH / 2\n",
        "            image_data_pre = np.clip(image_data, lower_bound, upper_bound)\n",
        "            image_data_pre = (\n",
        "                (image_data_pre - np.min(image_data_pre))\n",
        "                / (np.max(image_data_pre) - np.min(image_data_pre))\n",
        "                * 255.0\n",
        "            )\n",
        "        else:\n",
        "            lower_bound, upper_bound = np.percentile(\n",
        "                image_data[image_data > 0], 0.5\n",
        "            ), np.percentile(image_data[image_data > 0], 99.5)\n",
        "            image_data_pre = np.clip(image_data, lower_bound, upper_bound)\n",
        "            image_data_pre = (\n",
        "                (image_data_pre - np.min(image_data_pre))\n",
        "                / (np.max(image_data_pre) - np.min(image_data_pre))\n",
        "                * 255.0\n",
        "            )\n",
        "            image_data_pre[image_data == 0] = 0\n",
        "\n",
        "        image_data_pre = np.uint8(image_data_pre)\n",
        "        img_roi = image_data_pre[z_index, :, :]\n",
        "        np.savez_compressed(join(npy_path, prefix + gt_name.split(gt_name_suffix)[0]+'.npz'), imgs=img_roi, gts=gt_roi, spacing=img_sitk.GetSpacing())\n",
        "        # save the image and ground truth as nii files for sanity check;\n",
        "        # they can be removed\n",
        "        img_roi_sitk = sitk.GetImageFromArray(img_roi)\n",
        "        gt_roi_sitk = sitk.GetImageFromArray(gt_roi)\n",
        "        sitk.WriteImage(\n",
        "            img_roi_sitk,\n",
        "            join(npy_path, prefix + gt_name.split(gt_name_suffix)[0] + \"_img.nii.gz\"),\n",
        "        )\n",
        "        sitk.WriteImage(\n",
        "            gt_roi_sitk,\n",
        "            join(npy_path, prefix + gt_name.split(gt_name_suffix)[0] + \"_gt.nii.gz\"),\n",
        "        )\n",
        "        # save the each CT image as npy file\n",
        "        for i in range(img_roi.shape[0]):\n",
        "            img_i = img_roi[i, :, :]\n",
        "            img_3c = np.repeat(img_i[:, :, None], 3, axis=-1)\n",
        "            resize_img_skimg = transform.resize(\n",
        "                img_3c,\n",
        "                (image_size, image_size),\n",
        "                order=3,\n",
        "                preserve_range=True,\n",
        "                mode=\"constant\",\n",
        "                anti_aliasing=True,\n",
        "            )\n",
        "            resize_img_skimg_01 = (resize_img_skimg - resize_img_skimg.min()) / np.clip(\n",
        "                resize_img_skimg.max() - resize_img_skimg.min(), a_min=1e-8, a_max=None\n",
        "            )  # normalize to [0, 1], (H, W, 3)\n",
        "            gt_i = gt_roi[i, :, :]\n",
        "            resize_gt_skimg = transform.resize(\n",
        "                gt_i,\n",
        "                (image_size, image_size),\n",
        "                order=0,\n",
        "                preserve_range=True,\n",
        "                mode=\"constant\",\n",
        "                anti_aliasing=False,\n",
        "            )\n",
        "            resize_gt_skimg = np.uint8(resize_gt_skimg)\n",
        "            assert resize_img_skimg_01.shape[:2] == resize_gt_skimg.shape\n",
        "            np.save(\n",
        "                join(\n",
        "                    npy_path,\n",
        "                    \"imgs\",\n",
        "                    prefix\n",
        "                    + gt_name.split(gt_name_suffix)[0]\n",
        "                    + \"-\"\n",
        "                    + str(i).zfill(3)\n",
        "                    + \".npy\",\n",
        "                ),\n",
        "                resize_img_skimg_01,\n",
        "            )\n",
        "            np.save(\n",
        "                join(\n",
        "                    npy_path,\n",
        "                    \"gts\",\n",
        "                    prefix\n",
        "                    + gt_name.split(gt_name_suffix)[0]\n",
        "                    + \"-\"\n",
        "                    + str(i).zfill(3)\n",
        "                    + \".npy\",\n",
        "                ),\n",
        "                resize_gt_skimg,\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OChOlD8Ylp5I"
      },
      "outputs": [],
      "source": [
        "!pip install -U monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkc_jj3KHTb8"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "train the image encoder and mask decoder\n",
        "freeze prompt image encoder\n",
        "\"\"\"\n",
        "\n",
        "# %% setup environment\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "join = os.path.join\n",
        "from tqdm import tqdm\n",
        "from skimage import transform\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import monai\n",
        "from monai.losses import DiceLoss\n",
        "from segment_anything import sam_model_registry\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "import random\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# set seeds\n",
        "torch.manual_seed(2023)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# torch.distributed.init_process_group(backend=\"gloo\")\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"4\"  # export OMP_NUM_THREADS=4\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"  # export OPENBLAS_NUM_THREADS=4\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"6\"  # export MKL_NUM_THREADS=6\n",
        "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"  # export VECLIB_MAXIMUM_THREADS=4\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\"  # export NUMEXPR_NUM_THREADS=6\n",
        "\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([251 / 255, 252 / 255, 30 / 255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle((x0, y0), w, h, edgecolor=\"blue\", facecolor=(0, 0, 0, 0), lw=2)\n",
        "    )\n",
        "\n",
        "def visualize_segmentation(image, gt_mask, pred_mask, epoch, step, file_path):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axs[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
        "    axs[0].set_title('Input Image')\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    axs[1].imshow(gt_mask.cpu().numpy(), cmap='gray')\n",
        "    axs[1].set_title('Ground Truth Mask')\n",
        "    axs[1].axis('off')\n",
        "\n",
        "    axs[2].imshow(pred_mask.detach().cpu().numpy(), cmap='gray')\n",
        "    axs[2].set_title('Predicted Mask')\n",
        "    axs[2].axis('off')\n",
        "\n",
        "    plt.suptitle(f'Epoch {epoch}, Step {step}')\n",
        "    plt.savefig(file_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "class NpyDataset(Dataset):\n",
        "    def __init__(self, data_root, bbox_shift=20):\n",
        "        self.data_root = data_root\n",
        "        self.gt_path = join(data_root, \"gts\")\n",
        "        self.img_path = join(data_root, \"imgs\")\n",
        "        self.gt_path_files = sorted(\n",
        "            glob.glob(join(self.gt_path, \"**/*.npy\"), recursive=True)\n",
        "        )\n",
        "        self.gt_path_files = [\n",
        "            file\n",
        "            for file in self.gt_path_files\n",
        "            if os.path.isfile(join(self.img_path, os.path.basename(file)))\n",
        "        ]\n",
        "        self.bbox_shift = bbox_shift\n",
        "        print(f\"number of images: {len(self.gt_path_files)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.gt_path_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load npy image (1024, 1024, 3), [0,1]\n",
        "        img_name = os.path.basename(self.gt_path_files[index])\n",
        "        img_1024 = np.load(\n",
        "            join(self.img_path, img_name), \"r\", allow_pickle=True\n",
        "        )  # (1024, 1024, 3)\n",
        "        # convert the shape to (3, H, W)\n",
        "        img_1024 = np.transpose(img_1024, (2, 0, 1))\n",
        "        assert (\n",
        "            np.max(img_1024) <= 1.0 and np.min(img_1024) >= 0.0\n",
        "        ), \"image should be normalized to [0, 1]\"\n",
        "        gt = np.load(\n",
        "            self.gt_path_files[index], \"r\", allow_pickle=True\n",
        "        )  # multiple labels [0, 1,4,5...], (256,256)\n",
        "        assert img_name == os.path.basename(self.gt_path_files[index]), (\n",
        "            \"img gt name error\" + self.gt_path_files[index] + self.npy_files[index]\n",
        "        )\n",
        "        label_ids = np.unique(gt)[1:]\n",
        "        gt2D = np.uint8(\n",
        "            gt == random.choice(label_ids.tolist())\n",
        "        )  # only one label, (256, 256)\n",
        "        assert np.max(gt2D) == 1 and np.min(gt2D) == 0.0, \"ground truth should be 0, 1\"\n",
        "        y_indices, x_indices = np.where(gt2D > 0)\n",
        "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "        # add perturbation to bounding box coordinates\n",
        "        H, W = gt2D.shape\n",
        "        x_min = max(0, x_min - random.randint(0, self.bbox_shift))\n",
        "        x_max = min(W, x_max + random.randint(0, self.bbox_shift))\n",
        "        y_min = max(0, y_min - random.randint(0, self.bbox_shift))\n",
        "        y_max = min(H, y_max + random.randint(0, self.bbox_shift))\n",
        "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
        "        return (\n",
        "            torch.tensor(img_1024).float(),\n",
        "            torch.tensor(gt2D[None, :, :]).long(),\n",
        "            torch.tensor(bboxes).float(),\n",
        "            img_name,\n",
        "        )\n",
        "\n",
        "\n",
        "# %% sanity test of dataset class\n",
        "tr_dataset = NpyDataset(\"/content/drive/MyDrive/npy/CT_Abd\")\n",
        "tr_dataloader = DataLoader(tr_dataset, batch_size=8, shuffle=True)\n",
        "for step, (image, gt, bboxes, names_temp) in enumerate(tr_dataloader):\n",
        "    print(image.shape, gt.shape, bboxes.shape)\n",
        "    # show the example\n",
        "    _, axs = plt.subplots(1, 2, figsize=(25, 25))\n",
        "    idx = random.randint(0, 7)\n",
        "    axs[0].imshow(image[idx].cpu().permute(1, 2, 0).numpy())\n",
        "    show_mask(gt[idx].cpu().numpy(), axs[0])\n",
        "    show_box(bboxes[idx].numpy(), axs[0])\n",
        "    axs[0].axis(\"off\")\n",
        "    # set title\n",
        "    axs[0].set_title(names_temp[idx])\n",
        "    idx = random.randint(0, 7)\n",
        "    axs[1].imshow(image[idx].cpu().permute(1, 2, 0).numpy())\n",
        "    show_mask(gt[idx].cpu().numpy(), axs[1])\n",
        "    show_box(bboxes[idx].numpy(), axs[1])\n",
        "    axs[1].axis(\"off\")\n",
        "    # set title\n",
        "    axs[1].set_title(names_temp[idx])\n",
        "    # plt.show()\n",
        "    plt.subplots_adjust(wspace=0.01, hspace=0)\n",
        "    plt.savefig(\"/content/drive/MyDrive/data_sanitycheck2.png\", bbox_inches=\"tight\", dpi=300)\n",
        "    plt.close()\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "class Arguments:\n",
        "    def __init__(self):\n",
        "        self.tr_npy_path = \"/content/drive/MyDrive/npy/CT_Abd\"\n",
        "        self.task_name = \"MedHQSAM-ViT-L\"\n",
        "        self.model_type = \"vit_l\"\n",
        "        self.checkpoint = \"/content/drive/MyDrive/sam_hq_vit_l.pth\"\n",
        "        self.load_pretrain = True  # Previously monitored with wandb\n",
        "        self.pretrain_model_path = \"\"\n",
        "        self.work_dir = \"/content/drive/MyDrive/work_dir\"\n",
        "        self.num_epochs = 10\n",
        "        self.batch_size = 2\n",
        "        self.num_workers = 0\n",
        "        self.weight_decay = 0.01\n",
        "        self.lr = 0.0001\n",
        "        self.use_wandb = False\n",
        "        self.use_amp = False\n",
        "        self.resume = \"\"\n",
        "        self.device = \"cuda\"  # or 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Using the class\n",
        "args = Arguments()\n",
        "\n",
        "\n",
        "if args.use_wandb:\n",
        "    import wandb\n",
        "\n",
        "    wandb.login()\n",
        "    wandb.init(\n",
        "        project=args.task_name,\n",
        "        config={\n",
        "            \"lr\": args.lr,\n",
        "            \"batch_size\": args.batch_size,\n",
        "            \"data_path\": args.tr_npy_path,\n",
        "            \"model_type\": args.model_type,\n",
        "        },\n",
        "    )\n",
        "\n",
        "# %% set up model for training\n",
        "# device = args.device\n",
        "run_id = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "model_save_path = \"/content/drive/MyDrive\"\n",
        "device = torch.device(args.device)\n",
        "# %% set up model\n",
        "\n",
        "\n",
        "class MedHQSAM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_encoder,\n",
        "        mask_decoder,\n",
        "        prompt_encoder,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = image_encoder\n",
        "        self.mask_decoder = mask_decoder\n",
        "        self.prompt_encoder = prompt_encoder\n",
        "        # freeze prompt encoder\n",
        "        for param in self.prompt_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, image, box):\n",
        "        image_embedding, interm_embeddings = self.image_encoder(image)  # (B, 256, 64, 64)\n",
        "        interm_embeddings = interm_embeddings[0].unsqueeze(0)\n",
        "\n",
        "        # do not compute gradients for prompt encoder\n",
        "        with torch.no_grad():\n",
        "            box_torch = torch.as_tensor(box, dtype=torch.float32, device=image.device)\n",
        "            if len(box_torch.shape) == 2:\n",
        "                box_torch = box_torch[:, None, :]  # (B, 1, 4)\n",
        "\n",
        "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
        "                points=None,\n",
        "                boxes=box_torch,\n",
        "                masks=None,\n",
        "            )\n",
        "        low_res_masks, _ = self.mask_decoder(\n",
        "            image_embeddings=image_embedding,  # (B, 256, 64, 64)\n",
        "            image_pe=self.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
        "            sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
        "            dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
        "            multimask_output=False,\n",
        "            hq_token_only=False,\n",
        "            interm_embeddings= interm_embeddings,\n",
        "        )\n",
        "        ori_res_masks = F.interpolate(\n",
        "            low_res_masks,\n",
        "            size=(image.shape[2], image.shape[3]),\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False,\n",
        "        )\n",
        "        return ori_res_masks\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    if not os.path.exists(model_save_path):\n",
        "        os.makedirs(model_save_path)\n",
        "\n",
        "    sam_model = sam_model_registry[args.model_type](checkpoint=args.checkpoint)\n",
        "    medsam_model = MedHQSAM(\n",
        "        image_encoder=sam_model.image_encoder,\n",
        "        mask_decoder=sam_model.mask_decoder,\n",
        "        prompt_encoder=sam_model.prompt_encoder,\n",
        "    ).to(device)\n",
        "    sam.train()\n",
        "\n",
        "    print(\n",
        "        \"Number of total parameters: \",\n",
        "        sum(p.numel() for p in medsam_model.parameters()),\n",
        "    )  # 93735472\n",
        "    print(\n",
        "        \"Number of trainable parameters: \",\n",
        "        sum(p.numel() for p in medsam_model.parameters() if p.requires_grad),\n",
        "    )  # 93729252\n",
        "\n",
        "    img_mask_encdec_params = list(medsam_model.image_encoder.parameters()) + list(\n",
        "        medsam_model.mask_decoder.parameters()\n",
        "    )\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        img_mask_encdec_params, lr=args.lr, weight_decay=args.weight_decay\n",
        "    )\n",
        "    print(\n",
        "        \"Number of image encoder and mask decoder parameters: \",\n",
        "        sum(p.numel() for p in img_mask_encdec_params if p.requires_grad),\n",
        "    )  # 93729252\n",
        "    seg_loss = DiceLoss(sigmoid=True, squared_pred=True, reduction=\"mean\")\n",
        "    # cross entropy loss\n",
        "    ce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
        "    # %% train\n",
        "    num_epochs = args.num_epochs\n",
        "    iter_num = 0\n",
        "    losses = []\n",
        "    best_loss = 1e10\n",
        "    train_dataset = NpyDataset(args.tr_npy_path)\n",
        "\n",
        "    print(\"Number of training samples: \", len(train_dataset))\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    start_epoch = 0\n",
        "    if args.resume is not None:\n",
        "        if os.path.isfile(args.resume):\n",
        "            ## Map model to be loaded to specified single GPU\n",
        "            checkpoint = torch.load(args.resume, map_location=device)\n",
        "            start_epoch = checkpoint[\"epoch\"] + 1\n",
        "            medsam_model.load_state_dict(checkpoint[\"model\"])\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    if args.use_amp:\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for step, (image, gt2D, boxes, _) in enumerate(tqdm(train_dataloader)):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            boxes_np = boxes.detach().cpu().numpy()\n",
        "            image, gt2D = image.to(device), gt2D.to(device)\n",
        "            if args.use_amp:\n",
        "                ## AMP\n",
        "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                    medsam_pred = medsam_model(image, boxes_np)\n",
        "                    loss = seg_loss(medsam_pred, gt2D) + ce_loss(\n",
        "                        medsam_pred, gt2D.float()\n",
        "                    )\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "            else:\n",
        "                medsam_pred = medsam_model(image, boxes_np)\n",
        "                loss = seg_loss(medsam_pred, gt2D) + ce_loss(medsam_pred, gt2D.float())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            iter_num += 1\n",
        "\n",
        "            if step % 1200 == 0:  # Adjust the frequency as needed\n",
        "              with torch.no_grad():\n",
        "                  sample_idx = random.randint(0, image.size(0) - 1)\n",
        "                  pred_mask = torch.sigmoid(medsam_pred[sample_idx, 0])\n",
        "                  visualize_segmentation(\n",
        "                      image=image[sample_idx],\n",
        "                      gt_mask=gt2D[sample_idx, 0],\n",
        "                      pred_mask=(pred_mask > 0.5).float(),\n",
        "                      epoch=epoch,\n",
        "                      step=step,\n",
        "                      file_path=os.path.join(model_save_path, f'visualization_epoch_{epoch}_step_{step}.png')\n",
        "                  )\n",
        "\n",
        "        epoch_loss /= step\n",
        "        losses.append(epoch_loss)\n",
        "        if args.use_wandb:\n",
        "            wandb.log({\"epoch_loss\": epoch_loss})\n",
        "        print(\n",
        "            f'Time: {datetime.now().strftime(\"%Y%m%d-%H%M\")}, Epoch: {epoch}, Loss: {epoch_loss}'\n",
        "        )\n",
        "        ## save the latest model\n",
        "        checkpoint = {\n",
        "            \"model\": medsam_model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "        }\n",
        "        torch.save(checkpoint, join(model_save_path, \"med_hq-sam_model_latest.pth\"))\n",
        "        ## save the best model\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            checkpoint = {\n",
        "                \"model\": medsam_model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "            }\n",
        "            torch.save(checkpoint, join(model_save_path, \"med_hq-sam_model_best.pth\"))\n",
        "\n",
        "        # %% plot loss\n",
        "        plt.plot(losses)\n",
        "        plt.title(\"Dice + Cross Entropy Loss\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.savefig(join(model_save_path, args.task_name + \"train_loss_medhqsam.png\"))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1OX29mh-6dvOKQtDq0ASjhsxRqaEOpC6H",
      "authorship_tag": "ABX9TyNr9eKk/eRkF2/gM+8OCWkZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}