{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVtU0aW3VRK5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "\n",
        "!git clone https://github.com/gttae/med-hq-sam.git\n",
        "!pip install timm\n",
        "os.chdir('med-hq-sam')\n",
        "!export PYTHONPATH=$(pwd)\n",
        "from segment_anything import sam_model_registry, SamPredictor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U monai"
      ],
      "metadata": {
        "id": "AC5hQ4VtVSb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn as nn\n",
        "import glob\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "join = os.path.join\n",
        "\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([251 / 255, 252 / 255, 30 / 255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle((x0, y0), w, h, edgecolor=\"blue\", facecolor=(0, 0, 0, 0), lw=2)\n",
        "    )\n",
        "\n",
        "def visualize_segmentation(image, gt_mask, pred_mask, epoch, step, file_path):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axs[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
        "    axs[0].set_title('Input Image')\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    axs[1].imshow(gt_mask.cpu().numpy(), cmap='gray')\n",
        "    axs[1].set_title('Ground Truth Mask')\n",
        "    axs[1].axis('off')\n",
        "\n",
        "    axs[2].imshow(pred_mask.detach().cpu().numpy(), cmap='gray')\n",
        "    axs[2].set_title('Predicted Mask')\n",
        "    axs[2].axis('off')\n",
        "\n",
        "    plt.suptitle(f'Epoch {epoch}, Step {step}')\n",
        "    plt.savefig(file_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "class NpyDataset(Dataset):\n",
        "    def __init__(self, data_root, bbox_shift=20):\n",
        "        self.data_root = data_root\n",
        "        self.gt_path = join(data_root, \"gts\")\n",
        "        self.img_path = join(data_root, \"imgs\")\n",
        "        self.gt_path_files = sorted(\n",
        "            glob.glob(join(self.gt_path, \"**/*.npy\"), recursive=True)\n",
        "        )\n",
        "        self.gt_path_files = [\n",
        "            file\n",
        "            for file in self.gt_path_files\n",
        "            if os.path.isfile(join(self.img_path, os.path.basename(file)))\n",
        "        ]\n",
        "        self.bbox_shift = bbox_shift\n",
        "        print(f\"number of images: {len(self.gt_path_files)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.gt_path_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load npy image (1024, 1024, 3), [0,1]\n",
        "        img_name = os.path.basename(self.gt_path_files[index])\n",
        "        img_1024 = np.load(\n",
        "            join(self.img_path, img_name), \"r\", allow_pickle=True\n",
        "        )  # (1024, 1024, 3)\n",
        "        # convert the shape to (3, H, W)\n",
        "        img_1024 = np.transpose(img_1024, (2, 0, 1))\n",
        "        assert (\n",
        "            np.max(img_1024) <= 1.0 and np.min(img_1024) >= 0.0\n",
        "        ), \"image should be normalized to [0, 1]\"\n",
        "        gt = np.load(\n",
        "            self.gt_path_files[index], \"r\", allow_pickle=True\n",
        "        )  # multiple labels [0, 1,4,5...], (256,256)\n",
        "        assert img_name == os.path.basename(self.gt_path_files[index]), (\n",
        "            \"img gt name error\" + self.gt_path_files[index] + self.npy_files[index]\n",
        "        )\n",
        "        label_ids = np.unique(gt)[1:]\n",
        "        gt2D = np.uint8(\n",
        "            gt == random.choice(label_ids.tolist())\n",
        "        )  # only one label, (256, 256)\n",
        "        assert np.max(gt2D) == 1 and np.min(gt2D) == 0.0, \"ground truth should be 0, 1\"\n",
        "        y_indices, x_indices = np.where(gt2D > 0)\n",
        "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "        # add perturbation to bounding box coordinates\n",
        "        H, W = gt2D.shape\n",
        "        x_min = max(0, x_min - random.randint(0, self.bbox_shift))\n",
        "        x_max = min(W, x_max + random.randint(0, self.bbox_shift))\n",
        "        y_min = max(0, y_min - random.randint(0, self.bbox_shift))\n",
        "        y_max = min(H, y_max + random.randint(0, self.bbox_shift))\n",
        "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
        "        return (\n",
        "            torch.tensor(img_1024).float(),\n",
        "            torch.tensor(gt2D[None, :, :]).long(),\n",
        "            torch.tensor(bboxes).float(),\n",
        "            img_name,\n",
        "        )\n",
        "\n",
        "# Initialize your evaluation dataset\n",
        "eval_dataset = NpyDataset(\"/content/drive/MyDrive/npy3/CT_Abd/\")\n",
        "\n",
        "# Initialize your data loader for evaluation\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "\n",
        "class MedSAM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_encoder,\n",
        "        mask_decoder,\n",
        "        prompt_encoder,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = image_encoder\n",
        "        self.mask_decoder = mask_decoder\n",
        "        self.prompt_encoder = prompt_encoder\n",
        "        # freeze prompt encoder\n",
        "        for param in self.prompt_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, image, box):\n",
        "        image_embedding, interm_embeddings = self.image_encoder(image)  # (B, 256, 64, 64)\n",
        "        interm_embeddings = interm_embeddings[0].unsqueeze(0)\n",
        "\n",
        "        # do not compute gradients for prompt encoder\n",
        "        with torch.no_grad():\n",
        "            box_torch = torch.as_tensor(box, dtype=torch.float32, device=image.device)\n",
        "            if len(box_torch.shape) == 2:\n",
        "                box_torch = box_torch[:, None, :]  # (B, 1, 4)\n",
        "\n",
        "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
        "                points=None,\n",
        "                boxes=box_torch,\n",
        "                masks=None,\n",
        "            )\n",
        "        low_res_masks, _ = self.mask_decoder(\n",
        "            image_embeddings=image_embedding,  # (B, 256, 64, 64)\n",
        "            image_pe=self.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
        "            sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
        "            dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
        "            multimask_output=False,\n",
        "            hq_token_only=False,\n",
        "            interm_embeddings= interm_embeddings,\n",
        "        )\n",
        "        ori_res_masks = F.interpolate(\n",
        "            low_res_masks,\n",
        "            size=(image.shape[2], image.shape[3]),\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False,\n",
        "        )\n",
        "        return ori_res_masks\n",
        "\n",
        "# Load your model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sam_model = sam_model_registry['vit_l'](checkpoint=\"/content/drive/MyDrive/med_hq-sam_model_best.pth\")\n",
        "medsam_model = MedSAM(\n",
        "    image_encoder=sam_model.image_encoder,\n",
        "    mask_decoder=sam_model.mask_decoder,\n",
        "    prompt_encoder=sam_model.prompt_encoder,\n",
        ").to(device)\n",
        "\n",
        "# Load the best model for evaluation\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/med_hq-sam_model_best.pth\", map_location=device)\n",
        "medsam_model.load_state_dict(checkpoint[\"model\"])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "medsam_model.eval()\n",
        "\n",
        "# Define the Dice coefficient calculation function\n",
        "def dice_coefficient(prediction, target):\n",
        "    smooth = 1e-5\n",
        "    intersection = (prediction * target).sum()\n",
        "    dice = (2. * intersection + smooth) / (prediction.sum() + target.sum() + smooth)\n",
        "    return dice.item()\n",
        "\n",
        "# Initialize lists to store results\n",
        "dice_scores = []\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    for image, gt2D, boxes, _ in tqdm(eval_dataloader):\n",
        "        image, gt2D = image.to(device), gt2D.to(device)\n",
        "        boxes_np = boxes.detach().cpu().numpy()\n",
        "\n",
        "        # Forward pass\n",
        "        medsam_pred = medsam_model(image, boxes_np)\n",
        "\n",
        "        # Threshold the predicted masks\n",
        "        pred_mask = (torch.sigmoid(medsam_pred) > 0.5).float()\n",
        "\n",
        "        # Calculate Dice score for each sample in the batch\n",
        "        for i in range(len(pred_mask)):\n",
        "            dice = dice_coefficient(pred_mask[i, 0], gt2D[i, 0])\n",
        "            dice_scores.append(dice)\n",
        "\n",
        "        for i in range(len(pred_mask)):\n",
        "            file_path = f\"/content/drive/MyDrive/medhqsam_result/segmentation_result_{[i]}.png\"\n",
        "            visualize_segmentation(image[i], gt2D[i, 0], pred_mask[i, 0], 0, 0, file_path)\n",
        "\n",
        "# Calculate the average Dice score\n",
        "average_dice_score = np.mean(dice_scores)\n",
        "print(\"Average Dice Score:\", average_dice_score)"
      ],
      "metadata": {
        "id": "DzHKuNp8VWGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}